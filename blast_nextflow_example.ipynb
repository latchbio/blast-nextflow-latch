{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAk_IL8h7MeT"
   },
   "source": [
    "# Tutorial: Running Nextflow in Python with Latch SDK\n",
    "\n",
    "Estimated time to complete: 45 minutes\n",
    "\n",
    "Nextflow is a popular framework for orchestrating bioinformatics workflows. In this tutorial, we will walk through how you can package an existing Nextflow script in Latch Python SDK!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we start, make sure you:\n",
    "\n",
    "* Install the [Latch SDK](../getting_started/quick_start.md).\n",
    "* Understand basic concepts of a workflow through our [Quickstart](../getting_started/quick_start.md) and [Authoring your Own Workflow](../getting_started/authoring_your_workflow.md).\n",
    "\n",
    "As an example, we will use a BLAST workflow written in Nextflow. Let's dive in!\n",
    "\n",
    "### Step 1: Install the Tutorial GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0bSChjjh67p3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'blast-nextflow-latch'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 23 (delta 0), reused 23 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (23/23), 8.34 KiB | 2.78 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/latchbio/blast-nextflow-latch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_316_jq9HKT"
   },
   "source": [
    "At a high level, the repo contains the original BLAST Nextflow Pipeline, as well as additional files and folders required to upload the workflow to Latch:\n",
    "\n",
    "<a href=\"https://ibb.co/pLDVxRK\"><img src=\"https://i.ibb.co/jrK0TWw/latch-nextflow-structure.png\" alt=\"latch-nextflow-structure\" border=\"0\" /></a>\n",
    "\n",
    "We will first attempt to run the Nextflow pipeline locally, and walk through the additional files required to package the pipeline and upload it to Latch.\n",
    "\n",
    "## Step 2: Install dependencies for the Nextflow Pipeline\n",
    "\n",
    "To successfully run Nextflow BLAST pipeline inside the Pod, your Pod environment needs:\n",
    "\n",
    "* Java 8\n",
    "* Nextflow (version 20.07.x or higher)\n",
    "\n",
    "In this tutorial, we will use `conda` to manage the pipeline dependencies. We need to additionally install:\n",
    "\n",
    "* Micromamba (a faster alternative to Anaconda)\n",
    "* BLAST\n",
    "\n",
    "Let's walk through our Jupyter Notebook to see how these dependencies are installed!\n",
    "\n",
    "> Note: All the commands below assume that you are in a Linux environment. Please ensure that you are inside a Latch Pod or an alternative Linux environment before proceeding.\n",
    "\n",
    "### Update system dependencies\n",
    "\n",
    "First, let's download and update existing dependencies on our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2hGZMWwc-Ffi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Hit:2 https://download.docker.com/linux/ubuntu focal InRelease                 \n",
      "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease     \n",
      "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease     \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1277 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.3 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1929 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2859 kB]\n",
      "Fetched 6433 kB in 3s (2379 kB/s)                          \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "curl is already the newest version (7.68.0-1ubuntu2.15).\n",
      "git is already the newest version (1:2.25.1-1ubuntu3.6).\n",
      "unzip is already the newest version (6.0-25ubuntu1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 78 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! apt-get update -y && apt-get install -y curl unzip git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-mE9dBT-HaX"
   },
   "source": [
    "### Install Java 8\n",
    "\n",
    "Java 8 is required to run Nextflow. You can install the headless version of Java 8 like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U49JqYkT-JhB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "default-jre-headless is already the newest version (2:1.11-72).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 78 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! apt-get install -y default-jre-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQSkZm24-LB6"
   },
   "source": [
    "### Install Nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zfyWym5E-Mfp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Knloading nextflow dependencies. It may require a few seconds, please wait .. Downloading nextflow dependencies. It may require a few seconds, please wait .. \n",
      "      N E X T F L O W\n",
      "      version 22.10.4 build 5836\n",
      "      created 09-12-2022 09:58 UTC (09:58 GMT)\n",
      "      cite doi:10.1038/nbt.3820\n",
      "      http://nextflow.io\n",
      "\n",
      "\n",
      "Nextflow installation completed. Please note:\n",
      "- the executable file `nextflow` has been created in the folder: /root\n",
      "- you may complete the installation by moving it to a directory in your $PATH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! curl -s https://get.nextflow.io | bash && \\\n",
    "    mv nextflow /usr/bin/ && \\\n",
    "    chmod 777 /usr/bin/nextflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUDpBCez-OaR"
   },
   "source": [
    "### Install Micromamba\n",
    "\n",
    "Micromamba is a drop-in replace for Conda that is faster and more light-weight. It uses the same commands and configurations as Conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bJhtsXXn-Q-I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://download.docker.com/linux/ubuntu focal InRelease\n",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease     \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
      "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "bzip2 is already the newest version (1.0.8-2).\n",
      "wget is already the newest version (1.20.3-1ubuntu2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 78 not upgraded.\n",
      "bin/micromamba\n",
      "Modifying RC file \"/root/.bashrc\"\n",
      "Generating config for root prefix \u001b[1m\"/opt/conda\"\u001b[0m\n",
      "Setting mamba executable to: \u001b[1m\"/root/bin/micromamba\"\u001b[0mAdding (or replacing) the following in your \"/root/.bashrc\" file\n",
      "\u001b[32m\n",
      "# >>> mamba initialize >>>\n",
      "# !! Contents within this block are managed by 'mamba init' !!\n",
      "export MAMBA_EXE=\"/root/bin/micromamba\";\n",
      "export MAMBA_ROOT_PREFIX=\"/opt/conda\";\n",
      "__mamba_setup=\"$(\"$MAMBA_EXE\" shell hook --shell bash --prefix \"$MAMBA_ROOT_PREFIX\" 2> /dev/null)\"\n",
      "if [ $? -eq 0 ]; then\n",
      "    eval \"$__mamba_setup\"\n",
      "else\n",
      "    if [ -f \"/opt/conda/etc/profile.d/micromamba.sh\" ]; then\n",
      "        . \"/opt/conda/etc/profile.d/micromamba.sh\"\n",
      "    else\n",
      "        export  PATH=\"/opt/conda/bin:$PATH\"  # extra space after export prevents interference from conda init\n",
      "    fi\n",
      "fi\n",
      "unset __mamba_setup\n",
      "# <<< mamba initialize <<<\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! export CONDA_DIR=/opt/conda\n",
    "! export MAMBA_ROOT_PREFIX=/opt/conda\n",
    "! export PATH=$CONDA_DIR/bin:$PATH\n",
    "\n",
    "! apt-get update && apt-get install -y wget bzip2 \\\n",
    "    && wget -qO-  https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba \\\n",
    "    && touch /root/.bashrc \\\n",
    "    && ./bin/micromamba shell init -s bash -p /opt/conda  \\\n",
    "    && grep -v '[ -z \"\\$PS1\" ] && return' /root/.bashrc  > /opt/conda/bashrc \\\n",
    "    && apt-get clean autoremove --yes \\\n",
    "    && rm -rf /var/lib/{apt,dpkg,cache,log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knsoXt-m-UP_"
   },
   "source": [
    "We can use YAML files to manage conda dependencies. Inspecting the `blast-nf` folder, there is a `conda.yml` file that specifies `blast`, which is the only dependency required for this pipeline.\n",
    "\n",
    "```yml\n",
    "# blast-nf/conda.yml\n",
    "name: blast-nf\n",
    "channels:\n",
    "  - defaults\n",
    "  - bioconda\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - blast\n",
    "```\n",
    "\n",
    "You can create an environment called `blast-nf` using Micromamba like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "j6YeucQ3-VN-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                           __\n",
      "          __  ______ ___  ____ _____ ___  / /_  ____ _\n",
      "         / / / / __ `__ \\/ __ `/ __ `__ \\/ __ \\/ __ `/\n",
      "        / /_/ / / / / / / /_/ / / / / / / /_/ / /_/ /\n",
      "       / .___/_/ /_/ /_/\\__,_/_/ /_/ /_/_.___/\\__,_/\n",
      "      /_/\n",
      "\n",
      "bioconda/linux-64                                           Using cache\n",
      "bioconda/noarch                                             Using cache\n",
      "conda-forge/linux-64                                        Using cache\n",
      "conda-forge/noarch                                          Using cache\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /root/micromamba/envs/blast-nf\n",
      "\n",
      "  Updating specs:\n",
      "\n",
      "   - blast\n",
      "\n",
      "\n",
      "  Package                         Version  Build             Channel                    Size\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  Install:\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  + _libgcc_mutex                     0.1  conda_forge       conda-forge/linux-64     Cached\n",
      "  + _openmp_mutex                     4.5  2_gnu             conda-forge/linux-64     Cached\n",
      "  + blast                          2.13.0  hf3cf87c_0        bioconda/linux-64        Cached\n",
      "  + bzip2                           1.0.8  h7b6447c_0        pkgs/main/linux-64       Cached\n",
      "  + c-ares                         1.18.1  h7f8727e_0        pkgs/main/linux-64       Cached\n",
      "  + ca-certificates            2022.10.11  h06a4308_0        pkgs/main/linux-64       Cached\n",
      "  + curl                           7.86.0  h5eee18b_0        pkgs/main/linux-64       Cached\n",
      "  + entrez-direct                    16.2  he881be0_1        bioconda/linux-64        Cached\n",
      "  + krb5                           1.19.2  hac12032_0        pkgs/main/linux-64       Cached\n",
      "  + libcurl                        7.86.0  h91b91d3_0        pkgs/main/linux-64       Cached\n",
      "  + libedit                  3.1.20221030  h5eee18b_0        pkgs/main/linux-64       Cached\n",
      "  + libev                            4.33  h7f8727e_1        pkgs/main/linux-64       Cached\n",
      "  + libgcc-ng                      12.2.0  h65d4601_19       conda-forge/linux-64     Cached\n",
      "  + libgomp                        12.2.0  h65d4601_19       conda-forge/linux-64     Cached\n",
      "  + libidn2                         2.3.2  h7f8727e_0        pkgs/main/linux-64       Cached\n",
      "  + libnghttp2                     1.46.0  hce63b2e_0        pkgs/main/linux-64       Cached\n",
      "  + libnsl                          2.0.0  h5eee18b_0        pkgs/main/linux-64       Cached\n",
      "  + libssh2                        1.10.0  h8f2d780_0        pkgs/main/linux-64       Cached\n",
      "  + libstdcxx-ng                   12.2.0  h46fd767_19       conda-forge/linux-64     Cached\n",
      "  + libunistring                   0.9.10  h27cfd23_0        pkgs/main/linux-64       Cached\n",
      "  + libzlib                        1.2.13  h166bdaf_4        conda-forge/linux-64     Cached\n",
      "  + ncurses                           6.3  h5eee18b_3        pkgs/main/linux-64       Cached\n",
      "  + openssl                        1.1.1s  h7f8727e_0        pkgs/main/linux-64       Cached\n",
      "  + pcre                             8.45  h295c915_0        pkgs/main/linux-64       Cached\n",
      "  + perl                           5.32.1  0_h5eee18b_perl5  pkgs/main/linux-64       Cached\n",
      "  + perl-archive-tar                 2.40  pl5321hdfd78af_0  bioconda/noarch          Cached\n",
      "  + perl-carp                        1.38  pl5321hdfd78af_4  bioconda/noarch          Cached\n",
      "  + perl-common-sense                3.75  pl5321hdfd78af_0  bioconda/noarch          Cached\n",
      "  + perl-compress-raw-bzip2         2.201  pl5321h87f3376_1  bioconda/linux-64        Cached\n",
      "  + perl-compress-raw-zlib          2.105  pl5321h87f3376_0  bioconda/linux-64        Cached\n",
      "  + perl-encode                      3.19  pl5321hec16e2b_1  bioconda/linux-64        Cached\n",
      "  + perl-exporter                    5.72  pl5321hdfd78af_2  bioconda/noarch          Cached\n",
      "  + perl-exporter-tiny           1.002002  pl5321hdfd78af_0  bioconda/noarch          Cached\n",
      "  + perl-extutils-makemaker          7.66  pl5321hd8ed1ab_0  conda-forge/noarch       Cached\n",
      "  + perl-io-compress                2.201  pl5321h87f3376_0  bioconda/linux-64        Cached\n",
      "  + perl-io-zlib                     1.12  pl5321hdfd78af_0  bioconda/noarch          Cached\n",
      "  + perl-json                        4.10  pl5321hdfd78af_0  bioconda/noarch          Cached\n",
      "  + perl-json-xs                     2.34  pl5321h9f5acd7_5  bioconda/linux-64        Cached\n",
      "  + perl-list-moreutils             0.430  pl5321hdfd78af_0  bioconda/noarch          Cached\n",
      "  + perl-list-moreutils-xs          0.430  pl5321hec16e2b_1  bioconda/linux-64        Cached\n",
      "  + perl-parent                     0.236  pl5321hdfd78af_2  bioconda/noarch          Cached\n",
      "  + perl-pathtools                   3.75  pl5321hec16e2b_3  bioconda/linux-64        Cached\n",
      "  + perl-scalar-list-utils           1.62  pl5321hec16e2b_1  bioconda/linux-64        Cached\n",
      "  + perl-types-serialiser            1.01  pl5321hdfd78af_0  bioconda/noarch          Cached\n",
      "  + wget                           1.21.3  h0b77cf5_0        pkgs/main/linux-64       Cached\n",
      "  + zlib                           1.2.13  h166bdaf_4        conda-forge/linux-64     Cached\n",
      "\n",
      "  Summary:\n",
      "\n",
      "  Install: 46 packages\n",
      "\n",
      "  Total download: 0 B\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "\n",
      "Transaction starting\n",
      "Linking _libgcc_mutex-0.1-conda_forge\n",
      "Linking libstdcxx-ng-12.2.0-h46fd767_19\n",
      "Linking libgomp-12.2.0-h65d4601_19\n",
      "Linking _openmp_mutex-4.5-2_gnu\n",
      "Linking libgcc-ng-12.2.0-h65d4601_19\n",
      "Linking libzlib-1.2.13-h166bdaf_4\n",
      "Linking zlib-1.2.13-h166bdaf_4\n",
      "Linking ca-certificates-2022.10.11-h06a4308_0\n",
      "Linking ncurses-6.3-h5eee18b_3\n",
      "Linking libev-4.33-h7f8727e_1\n",
      "Linking c-ares-1.18.1-h7f8727e_0\n",
      "Linking libunistring-0.9.10-h27cfd23_0\n",
      "Linking libnsl-2.0.0-h5eee18b_0\n",
      "Linking pcre-8.45-h295c915_0\n",
      "Linking bzip2-1.0.8-h7b6447c_0\n",
      "Linking openssl-1.1.1s-h7f8727e_0\n",
      "Linking libedit-3.1.20221030-h5eee18b_0\n",
      "Linking libidn2-2.3.2-h7f8727e_0\n",
      "Linking perl-5.32.1-0_h5eee18b_perl5\n",
      "Linking libssh2-1.10.0-h8f2d780_0\n",
      "Linking libnghttp2-1.46.0-hce63b2e_0\n",
      "Linking krb5-1.19.2-hac12032_0\n",
      "Linking wget-1.21.3-h0b77cf5_0\n",
      "Linking libcurl-7.86.0-h91b91d3_0\n",
      "Linking curl-7.86.0-h5eee18b_0\n",
      "Linking perl-extutils-makemaker-7.66-pl5321hd8ed1ab_0\n",
      "Linking perl-parent-0.236-pl5321hdfd78af_2\n",
      "Linking perl-exporter-5.72-pl5321hdfd78af_2\n",
      "Linking perl-common-sense-3.75-pl5321hdfd78af_0\n",
      "Linking perl-io-zlib-1.12-pl5321hdfd78af_0\n",
      "Linking perl-exporter-tiny-1.002002-pl5321hdfd78af_0\n",
      "Linking perl-carp-1.38-pl5321hdfd78af_4\n",
      "Linking perl-types-serialiser-1.01-pl5321hdfd78af_0\n",
      "Linking perl-scalar-list-utils-1.62-pl5321hec16e2b_1\n",
      "Linking perl-compress-raw-zlib-2.105-pl5321h87f3376_0\n",
      "Linking perl-compress-raw-bzip2-2.201-pl5321h87f3376_1\n",
      "Linking perl-list-moreutils-xs-0.430-pl5321hec16e2b_1\n",
      "Linking entrez-direct-16.2-he881be0_1\n",
      "Linking perl-encode-3.19-pl5321hec16e2b_1\n",
      "Linking perl-pathtools-3.75-pl5321hec16e2b_3\n",
      "Linking perl-json-xs-2.34-pl5321h9f5acd7_5\n",
      "Linking perl-io-compress-2.201-pl5321h87f3376_0\n",
      "Linking perl-list-moreutils-0.430-pl5321hdfd78af_0\n",
      "Linking perl-json-4.10-pl5321hdfd78af_0\n",
      "Linking perl-archive-tar-2.40-pl5321hdfd78af_0\n",
      "Linking blast-2.13.0-hf3cf87c_0\n",
      "Transaction finished\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/root/bin/micromamba create -f /root/blast-nextflow-latch/blast-nf/conda.yml -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUZ9OweX-ZcC"
   },
   "source": [
    "You can verify the dependencies installation by listing our the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "X0JZcMdK-bdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                           __\n",
      "          __  ______ ___  ____ _____ ___  / /_  ____ _\n",
      "         / / / / __ `__ \\/ __ `/ __ `__ \\/ __ \\/ __ `/\n",
      "        / /_/ / / / / / / /_/ / / / / / / /_/ / /_/ /\n",
      "       / .___/_/ /_/ /_/\\__,_/_/ /_/ /_/_.___/\\__,_/\n",
      "      /_/\n",
      "\n",
      "  Name       Active  Path                           \n",
      "──────────────────────────────────────────────────────\n",
      "  base               /root/micromamba               \n",
      "  blast              /root/micromamba/envs/blast    \n",
      "  blast-nf           /root/micromamba/envs/blast-nf \n",
      "  rnaseq-nf          /root/micromamba/envs/rnaseq-nf\n",
      "                     /root/miniconda                \n",
      "             *       /root/miniconda/envs/jupyterlab\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/root/bin/micromamba env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWfezknZ-fBB"
   },
   "source": [
    "Output on Latch Pod:\n",
    "\n",
    "```bash\n",
    "          __  ______ ___  ____ _____ ___  / /_  ____ _\n",
    "         / / / / __ `__ \\/ __ `/ __ `__ \\/ __ \\/ __ `/\n",
    "        / /_/ / / / / / / /_/ / / / / / / /_/ / /_/ /\n",
    "       / .___/_/ /_/ /_/\\__,_/_/ /_/ /_/_.___/\\__,_/\n",
    "      /_/\n",
    "\n",
    "  Name       Active  Path                           \n",
    "──────────────────────────────────────────────────────\n",
    "  base               /root/micromamba               \n",
    "  blast-nf              /root/micromamba/envs/blast-nf\n",
    "                     /root/miniconda                \n",
    "             *       /root/miniconda/envs/jupyterlab\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJITOq8G-h6x"
   },
   "source": [
    "### Run the BLAST Nextflow Pipeline\n",
    "\n",
    "Great! Now that we have successfully installed Nextflow and all required dependencies, let's run the BLAST pipeline locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UOTWtFiU-jku"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning  libmamba 'root_prefix' set with default value: /root/micromamba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 22.10.4\n",
      "Launching `/root/blast-example/main.nf` [prickly_baekeland] DSL2 - revision: 06c24a7542\n",
      "[-        ] process > blast   -\n",
      "[-        ] process > extract -\n",
      "\n",
      "executor >  local (1)\n",
      "[86/26851f] process > blast (1) [  0%] 0 of 1\n",
      "[-        ] process > extract   -\n",
      "\n",
      "executor >  local (2)\n",
      "[86/26851f] process > blast (1)   [100%] 1 of 1 ✔\n",
      "[f9/d54030] process > extract (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (2)\n",
      "[86/26851f] process > blast (1)   [100%] 1 of 1 ✔\n",
      "[f9/d54030] process > extract (1) [100%] 1 of 1 ✔\n",
      "\n",
      "executor >  local (2)\n",
      "[86/26851f] process > blast (1)   [100%] 1 of 1 ✔\n",
      "[f9/d54030] process > extract (1) [100%] 1 of 1 ✔\n",
      "matching sequences:\n",
      " >1ABO:B \n",
      "MNDPNLFVALYDFVASGDNTLSITKGEKLRVLGYNHNGEWCEAQTKNGQGWVPSNYITPVNS\n",
      ">1ABO:A \n",
      "MNDPNLFVALYDFVASGDNTLSITKGEKLRVLGYNHNGEWCEAQTKNGQGWVPSNYITPVNS\n",
      ">1YCS:B \n",
      "PEITGQVSLPPGKRTNLRKTGSERIAHGMRVKFNPLPLALLLDSSLEGEFDLVQRIIYEVDDPSLPNDEGITALHNAVCA\n",
      "GHTEIVKFLVQFGVNVNAADSDGWTPLHCAASCNNVQVCKFLVESGAAVFAMTYSDMQTAADKCEEMEEGYTQCSQFLYG\n",
      "VQEKMGIMNKGVIYALWDYEPQNDDELPMKEGDCMTIIHREDEDEIEWWWARLNDKEGYVPRNLLGLYPRIKPRQRSLA\n",
      ">1IHD:C \n",
      "LPNITILATGGTIAGGGDSATKSNYTVGKVGVENLVNAVPQLKDIANVKGEQVVNIGSQDMNDNVWLTLAKKINTDCDKT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/root/bin/micromamba run -n blast-nf /bin/bash -c \"nextflow run /root/blast-example/main.nf --out /root/results.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWVdUco1-l9k"
   },
   "source": [
    "The results would be output under `/root/results.txt`. You can see an example output file [here](https://console.latch.bio/s/594643294258238)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJi5sUPt-os2"
   },
   "source": [
    "## Step 3: Package the Nextflow Pipeline as a Latch Workflow\n",
    "\n",
    "<a href=\"https://ibb.co/pLDVxRK\"><img src=\"https://i.ibb.co/jrK0TWw/latch-nextflow-structure.png\" alt=\"latch-nextflow-structure\" border=\"0\" /></a>\n",
    "\n",
    "Now that we have successfully run the BLAST pipeline, let's walk through the additional files necessary to package it as a **Latch Workflow**. These files are:\n",
    "\n",
    "1. A Dockerfile to install the required dependencies\n",
    "2. A Python `__init__.py` to define workflow logic\n",
    "3. A `version` file to semantically name the workflow version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8kLv53hD-mo"
   },
   "source": [
    "### Define your Dockerfile to install dependencies\n",
    "\n",
    "As the Latch workflow will be executed on a fresh machine on the Latch platform, we have to define a Dockerfile with the necessary dependencies for BLAST to run.\n",
    "\n",
    "To do so, we can copy paste previous commands used to set up our environment:\n",
    "\n",
    "<a href=\"https://ibb.co/BqRwNHR\"><img src=\"https://i.ibb.co/dGy7gsy/nextflow-dockerfile.png\" alt=\"nextflow-dockerfile\" border=\"0\" /></a>\n",
    "\n",
    "* **Line 1**: is the [Latch base image](https://github.com/latchbio/latch-base), which is used to configure libraries required for consistent task behaviour.\n",
    "* **Line 3-4**: downloads and installs the updates for each outdated package and dependency on the machine that executes the workflow. `curl` and `unzip` are also installed.\n",
    "* **Line 7**: installs the Java runtime environment, which is required to run Nextflow.\n",
    "* **Line 8-10**: installs Nextflow and moves the binary to `/usr/bin`.\n",
    "* **Line 13-25**: is a series of commands to install Micromamba.\n",
    "* **Line 27**: copies the BLAST Nextflow pipeline code to the task execution environment. The `/root/blast-nf` is that path at which the NF code is stored in the machine that executes the task on Latch.\n",
    "* **Line 30**: uses Micromamba to install the dependencies as spefieid in `/root/blast-nf/conda.yml`.\n",
    "* **Line 35-39**: are already provided in the boilerplate Dockerfile and are needed to ensure your build envrionment works correctly with Latch.\n",
    "\n",
    "That's it! You've successfully defined your Dockerfile.\n",
    "\n",
    "To test whether the Dockerfile builds the correct environment, open a new terminal inside Jupyterlab and register your workflow like so:\n",
    "\n",
    "```bash\n",
    "eval `ssh-agent -s`\n",
    "\n",
    "latch register --remote blast-nextflow-latch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMX8GjyWET5q"
   },
   "source": [
    "Open a remote debugging session:\n",
    "\n",
    "```console\n",
    "latch develop .\n",
    "```\n",
    "\n",
    "Enter an interative shell:\n",
    "\n",
    "```console\n",
    ">>> shell\n",
    "\n",
    "Syncing local changes... \n",
    "Could not find /Users/hannahle/Documents/GitHub/nextflow-latch-wf/data - skipping\n",
    "Finished syncing.\n",
    "Pulling 812206152185.dkr.ecr.us-west-2.amazonaws.com/6064_nextflow-latch-wf:0.0.0-7da9b6... \n",
    "Image successfully pulled.\n",
    "```\n",
    "\n",
    "This will pull your workflow image built by the Dockerfile, which is handy to verify and reiterate on your build commands.\n",
    "\n",
    "For example, we can verify that Nextflow is installed correctly by typing:\n",
    "\n",
    "```console\n",
    "root@ip-10-0-11-243:~# nextflow\n",
    "Usage: nextflow [options] COMMAND [arg...]\n",
    "\n",
    "Options:\n",
    "  -C\n",
    "     Use the specified configuration file(s) overriding any defaults\n",
    "  -D\n",
    "     Set JVM properties\n",
    "  -bg\n",
    "     Execute nextflow in background\n",
    "  -c, -config\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb3R2ttxGHrO"
   },
   "source": [
    "### Define the Latch workflow\n",
    "\n",
    "The core logic of a Latch workflow is in the `wf/__init__.py`.\n",
    "\n",
    "To wrap the Nextflow workflow inside a Latch workflow, first import the necessary dependencies\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from latch import medium_task, workflow\n",
    "from latch.resources.launch_plan import LaunchPlan\n",
    "from latch.types import LatchAuthor, LatchFile, LatchMetadata, LatchParameter, LatchDir\n",
    "```\n",
    "\n",
    "Next, let's define our task:\n",
    "\n",
    "<a href=\"https://ibb.co/DC5qkvb\"><img src=\"https://i.ibb.co/LtnG6cP/blast-nf-wf.png\" alt=\"blast-nf-wf\" border=\"0\" /></a>\n",
    "\n",
    "* **Line 1**: specifies the compute that the RNASeq-NF pipeline needs. Here, we are using a `@small_task`, which will provision a machine with 2 cpus, 4 gigs of memory of memory to run the task. For a comprehensive list of all task resources available, visit [how to define cloud resources](./../basics/defining_cloud_resources.md).\n",
    "* **Line 3-8:** are the task parameters. We choose `query` and `db` because they are also required parameters in the BLAST Nextflow pipeline, as shown in Nextflow's `main.nf` file.\n",
    "\n",
    "<a href=\"https://ibb.co/CM9GF9z\"><img src=\"https://i.ibb.co/pRKFpKx/main-nf.png\" alt=\"main-nf\" border=\"0\" /></a>\n",
    "\n",
    "* **Line 10**: creates a filepath called `results.txt` that can be used to output the BLAST results to.\n",
    "* **Line 12**: `db.local_path` downloads the BLAST database to the task execution environment. We use Python list comprehension to retrieve all filenames under this directory.\n",
    "* **Line 14**: retrieves the common filename prefix across alls inside the BLAST database. This is necessary because the BLAST pipeline requires a common filename prefix to be appended to the BLAST database directory.\n",
    "* **Line 16-29**: specifies the command to be run by Python `subprocess` module.\n",
    "* **Line 17-22**: tells Micromamba to use the `blast-nf` conda environment previously installed in our Dockerfile.\n",
    "* **Line 24-27**: is the command to run the BLAST Nextflow pipeline with custom parameters.\n",
    "* **Line 31**: uses `subprocess` to pops open a process to execute the Nextflow command.\n",
    "* **Line 33**: takes the output `/root/results.txt` file and uploads it to [Latch Data](https://console.latch.bio/data) under a user-defined filename.\n",
    "\n",
    "Now you have successfully defined a Latch task with custom compute resources to execute the BLAST Nextflow pipeline on Latch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGaJo7P6HkAZ"
   },
   "source": [
    "### Calling a Latch task inside a Latch workflow\n",
    "\n",
    "Since this is a single task workflow, you can simply call the task inside the workflow and return its results like so:\n",
    "\n",
    "```python\n",
    "@workflow(metadata)\n",
    "def blast_wf(\n",
    "    query: LatchFile, db: LatchDir, out: str\n",
    ") -> LatchFile:\n",
    "    ...\n",
    "    return blast_task(query=query, db=db, out=out)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAtHX-vPHqWZ"
   },
   "source": [
    "### Defining Workflow GUI\n",
    "\n",
    "To expose workflow parameters to a user-friendly workflow GUI, you can use the `LatchMetadata` object. An important point to note is that all workflow arguments need to be added to the `parameters` key of LatchMetadata for them to display on the GUI. For an exhaustive list of how workflow argument and their Python types map to the front-end interface, visit [Customizing Your Interface](../basics/customizing_interface.md)\n",
    "\n",
    "```python\n",
    "\"\"\"The metadata included here will be injected into your interface.\"\"\"\n",
    "metadata = LatchMetadata(\n",
    "    display_name=\"Example: Wrapping a Nextflow BLAST Pipeline in Latch SDK\",\n",
    "    documentation=\"your-docs.dev\",\n",
    "    author=LatchAuthor(\n",
    "        name=\"John von Neumann\",\n",
    "        email=\"hungarianpapi4@gmail.com\",\n",
    "        github=\"github.com/fluid-dynamix\",\n",
    "    ),\n",
    "    repository=\"https://github.com/your-repo\",\n",
    "    license=\"MIT\",\n",
    "    parameters={\n",
    "        \"query\": LatchParameter(\n",
    "            display_name=\"FASTA File\",\n",
    "            description=\"Select FASTA file.\",\n",
    "            batch_table_column=True,  # Show this parameter in batched mode.\n",
    "        ),\n",
    "        \"db\": LatchParameter(\n",
    "            display_name=\"BLAST Database\",\n",
    "            description=\"Select the database to run BLAST against.\",\n",
    "            batch_table_column=True,  # Show this parameter in batched mode.\n",
    "        ),\n",
    "        \"out\": LatchParameter(\n",
    "            display_name=\"Output Text File\",\n",
    "            description=\"Specify the location of the output text file.\",\n",
    "            batch_table_column=True,  # Show this parameter in batched mode.\n",
    "        )\n",
    "    },\n",
    "    tags=[],\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H534bEMMHt8y"
   },
   "source": [
    "### Adding Test Data\n",
    "\n",
    "Finally, we can add some test data to run the workflow.\n",
    "\n",
    "In the BLAST Nextflow workflow, there is a folder for test data under `blast-nf/data` and an additional folder for BLAST database under `blast-nf/blast-db/pdb`. Let's upload these folders to a public S3 link, so that they can be used by others when running the workflow on Latch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eQiG9KqGHwAH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully uploaded test-data/4034/blast-nextflow-latch/blast-nf/data/sample.fa\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded to s3://latch-public/test-data/4034/blast-nextflow-latch/blast-nf/data\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded test-data/4034/blast-nextflow-latch/blast-nf/blast-db/pdb/tiny.pin\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded test-data/4034/blast-nextflow-latch/blast-nf/blast-db/pdb/tiny.pog\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded test-data/4034/blast-nextflow-latch/blast-nf/blast-db/pdb/tiny.psq\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded test-data/4034/blast-nextflow-latch/blast-nf/blast-db/pdb/tiny.phr\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded test-data/4034/blast-nextflow-latch/blast-nf/blast-db/pdb/tiny.psi\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded test-data/4034/blast-nextflow-latch/blast-nf/blast-db/pdb/tiny.psd\u001b[0m\n",
      "\u001b[32mSuccessfully uploaded to s3://latch-public/test-data/4034/blast-nextflow-latch/blast-nf/blast-db/pdb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! latch test-data upload blast-nextflow-latch/blast-nf/data\n",
    "! latch test-data upload blast-nextflow-latch/blast-nf/blast-db/pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dymmk29VHzIV"
   },
   "source": [
    "Once the command runs successfully, you will see the links at which the folder is uploaded. You can then use the `LaunchPlan` construct to add the remote files as test data like so:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Add test data with a LaunchPlan. Provide default values in a dictionary with\n",
    "the parameter names as the keys. These default values will be available under\n",
    "the 'Test Data' dropdown at console.latch.bio.\n",
    "\"\"\"\n",
    "LaunchPlan(\n",
    "    rnaseq_wf,\n",
    "    \"Test Data\",\n",
    "    {\n",
    "        \"reads\": [\n",
    "            LatchFile(\"s3://test-data/6064/rnaseq-nf/data/ggal/ggal_gut_1.fq\"), # <- Here we are using a different user's public S3 link - Substitute with your own if desired. Both will work.\n",
    "            LatchFile(\"s3://test-data/6064/rnaseq-nf/data/ggal/ggal_gut_2.fq\"),\n",
    "        ],\n",
    "        \"transcriptome\": LatchFile(\n",
    "            \"s3://test-data/6064/rnaseq-nf/data/ggal/ggal_1_48850000_49020000.Ggal71.500bpflank.fa\"\n",
    "        ),\n",
    "        \"outdir\": LatchDir(\"latch:///welcome\"),\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIrx_9iFH5So"
   },
   "source": [
    "### Registering the workflow to Latch Console\n",
    "\n",
    "To publish the workflow to the Latch platform, you can navigate to the root workflow directory and upload it with the `latch register` command:\n",
    "\n",
    "```\n",
    "latch register --remote blast-nextflow-latch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfy0TzRWH80r"
   },
   "source": [
    "This will give us:\n",
    "\n",
    "* a no-code interface\n",
    "* managed cloud infrastructure for workflow execution\n",
    "* a dedicated API endpoint for programmatic execution\n",
    "* hosted documentation\n",
    "* parallelized CSV-to-batch execution\n",
    "\n",
    "Once registration finishes, you can navigate to [Latch](https://console.latch.bio/workflows) to run your workflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhK5p1GuH-iT"
   },
   "source": [
    "\n",
    "## Commonly Asked Questions\n",
    "\n",
    "Below we aim to provide answers to the most commonly asked questions about porting a Nextflow pipeline to Latch:\n",
    "\n",
    "1. **Should I wrap an entire Nextflow pipeline in a single task or refactor each Nextflow process to an individual task?**\n",
    "\n",
    "    For prototyping purposes, we recommend that you wrap an entire Nextflow pipeline in a single task first. This allows you to quickly experience the development experience with a Pythonic SDK and publish a first workflow that's ready-to-use for scientists.\n",
    "\n",
    "    One disadvantage of this, however, is all processes are run on a single machine with fixed compute resource. If parallelization of individual processes across multiple machines is desired, it is beneficial to refactor each process into its individual task. With the [SDK's remote debugging toolkit](../basics/local_development.md), refactoring also enables for faster debugging and development.\n",
    "\n",
    "2. **Can I take advantage of existing Netxflow's community workflows while using the Latch SDK?**\n",
    "\n",
    "    Yes, absolutely! For example, say you want to run [NF-Core's demultiplex pipeline](https://nf-co.re/demultiplex), you can substitute the `nextflow run` command in our tutorial below with the following inside your Python subprocess:\n",
    "\n",
    "    ```console\n",
    "    nextflow run nf-core/demultiplex --input samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\n",
    "    ```\n",
    "\n",
    "3. **How does the SDK handle retries?**\n",
    "\n",
    "    Visit the documentation on how the SDK handles retries [here](https://docs.latch.bio/basics/retries.html). Currently, the SDK does not yet support autoscaling compute resources for failed tasks due to out-of-memory errors. This is a feature we're actively investigating and will release in future versions.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
